{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZc6wPwQYPnZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "import torch\n",
        "import json\n",
        "import tqdm as notebook_tqdm\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHIj0no1Y3xa"
      },
      "outputs": [],
      "source": [
        "def chunking(directory_path,tokenizer,chunk_size,para_separator=\"\\n\\n\",separator=\" \"):\n",
        "  \"\"\"\n",
        "  Split document content into chunks while preserving semantic meaning.\n",
        "\n",
        "  Args:\n",
        "    directory_path (str): Path to documents directory\n",
        "    tokenizer (str): Tokenizer model\n",
        "    chunk_size (int): Maximum tokens per chunk\n",
        "    para_separator (str): Paragraph separator\n",
        "    separator (str): Word separator\n",
        "\n",
        "  Returns:\n",
        "    dict: Document chunks with metadata\n",
        "  \"\"\"\n",
        "\n",
        "  #tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "  documents={}\n",
        "    \n",
        "  for filename in os.listdir(directory_path):\n",
        "    file_path=os.path.join(directory_path,filename)\n",
        "    if not os.path.isfile(file_path):\n",
        "      continue\n",
        "    \n",
        "    with open(file_path,'r',encoding='utf-8') as file:\n",
        "        text=file.read()\n",
        "    \n",
        "    base=os.path.basename(file_path)\n",
        "    sku=os.path.splitext(base)[0]\n",
        "    \n",
        "    doc_id=str(uuid.uuid4())\n",
        "    chunk_collection={}\n",
        "\n",
        "    for paragraph in re.split(para_separator,text):\n",
        "      current_chunk=[]\n",
        "      current_chunk_len=0\n",
        "      \n",
        "      for word in paragraph.split(separator):\n",
        "        word_tokens=len(tokenizer.tokenize(word))\n",
        "        if current_chunk_len+word_tokens<=chunk_size:\n",
        "          current_chunk.append(word)\n",
        "          current_chunk_len+=word_tokens\n",
        "        else:\n",
        "          if current_chunk:\n",
        "            chunk_text=separator.join(current_chunk)\n",
        "            chunk_id=str(uuid.uuid4())\n",
        "            chunk_collection[chunk_id]={\"TEXT\":chunk_text,\"metadata\": {\"filename\": sku}}\n",
        "          \n",
        "          current_chunk=[word]\n",
        "          current_chunk_len=word_tokens\n",
        "          \n",
        "      # Adding the remaining chunk\n",
        "      if current_chunk:\n",
        "        chunk_text=separator.join(current_chunk)\n",
        "        chunk_id=str(uuid.uuid4())\n",
        "        chunk_collection[chunk_id]={\"TEXT\":chunk_text,\"metadata\": {\"filename\": sku}}\n",
        "      \n",
        "    documents[doc_id]=chunk_collection\n",
        "    \n",
        "  return documents\n",
        "          \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_name= \"BAAI/llm-embedder\"\n",
        "\n",
        "# tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "# model=AutoModel.from_pretrained(model_name)\n",
        "\n",
        "def map_document_embeddings(documents,tokenizer,model_name):\n",
        "    mapped_document_db={}\n",
        "    model=AutoModel.from_pretrained(model_name)\n",
        "    model.to(\"cuda\")\n",
        "    \n",
        "    for id, dict_content in documents.items():\n",
        "        mapped_document_db[id]={}\n",
        "        for chunk_id, chunk_content in dict_content.items():\n",
        "            mapped_embeddings={}\n",
        "            text=chunk_content.get(\"TEXT\")\n",
        "            inputs=tokenizer(text,return_tensors=\"pt\",padding=True,truncation=True)\n",
        "            with torch.no_grad():\n",
        "                inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "                embeddings=model(**inputs).last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "            mapped_embeddings[chunk_id]=embeddings\n",
        "            mapped_document_db[id]=mapped_embeddings\n",
        "            \n",
        "    return mapped_document_db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_q_embeddings(query,tokenizer,model_name):\n",
        "    model=AutoModel.from_pretrained(model_name)\n",
        "    model.to(\"cuda\")\n",
        "    query_inputs=tokenizer(query,return_tensors=\"pt\",padding=True,truncation=True)\n",
        "    with torch.no_grad():\n",
        "        query_inputs = {k: v.to(\"cuda\") for k, v in query_inputs.items()}\n",
        "        query_embedding=model(**query_inputs).last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "    \n",
        "    return query_embedding\n",
        "\n",
        "def cosine_sim_score(query_embeddings,chunk_embeddings):\n",
        "    normalized_chunk=np.linalg.norm(chunk_embeddings)\n",
        "    normalized_query=np.linalg.norm(query_embeddings)\n",
        "            \n",
        "    if normalized_chunk==0 or normalized_query==0:\n",
        "        score=0\n",
        "    else:\n",
        "        score=np.dot(query_embeddings,chunk_embeddings)/(normalized_query*normalized_chunk)\n",
        "                \n",
        "    return score\n",
        "\n",
        "def get_top_k_scores(query_embeddings,mapped_document_db,top_k):\n",
        "    scores={} \n",
        "    \n",
        "    for doc_id, chunk_dict in mapped_document_db.items():\n",
        "        for chunk_id, chunk_embeddings in chunk_dict.items():\n",
        "            chunk_embedding=np.array(chunk_embeddings)\n",
        "            score=cosine_sim_score(query_embeddings,chunk_embeddings)\n",
        "            scores[(doc_id,chunk_id)]=score\n",
        "    \n",
        "    sorted_scores=sorted(scores.items(),key=lambda item: item[1],reverse=True)[:top_k]\n",
        "    \n",
        "    return sorted_scores\n",
        "\n",
        "def retrieve_top_results(sorted_scores):\n",
        "    top_results=[]\n",
        "    for((doc_id,chunk_id),score) in sorted_scores:\n",
        "        results=(doc_id, chunk_id,score)\n",
        "        top_results.append(results)\n",
        "    \n",
        "    return top_results \n",
        "\n",
        "def save_json(path,data):\n",
        "    with open(path,'w') as f:\n",
        "        json.dump(data,f,indent=4)\n",
        "        \n",
        "def read_json(path):\n",
        "    with open(path,'r') as f:\n",
        "        data=json.load(f)\n",
        "    \n",
        "    return data\n",
        "\n",
        "def retrieve_text(top_results, document):\n",
        "    first_match=top_results[0]\n",
        "    doc_id=first_match[0]\n",
        "    chunk_id=first_match[1]\n",
        "    related_text=document[doc_id][chunk_id]\n",
        "    \n",
        "    return related_text "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_llm_response(gemini_model,query,relevant_text):\n",
        "    prompt = f\"\"\"\n",
        "    You are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.\n",
        "\n",
        "    Your job is to understand the request, and answer based on the retrieved context.\n",
        "    Here is context:\n",
        "\n",
        "    <context>\n",
        "    {relevant_text[\"TEXT\"]}\n",
        "    </context>\n",
        "\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "    response = gemini_model.generate_content(prompt)\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "directory_path=\"documents\"\n",
        "\n",
        "embedding_model_name=\"BAAI/llm-embedder\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMBjrn9PXxr95iPU4xpI6x8",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
