{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fZc6wPwQYPnZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import re\n",
        "import uuid\n",
        "import torch\n",
        "import json\n",
        "import tqdm as notebook_tqdm\n",
        "import google.generativeai as genai\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wHIj0no1Y3xa"
      },
      "outputs": [],
      "source": [
        "def chunking(directory_path,tokenizer,chunk_size,para_separator=\"\\n\\n\",separator=\" \"):\n",
        "  \"\"\"\n",
        "  Split document content into chunks while preserving semantic meaning.\n",
        "\n",
        "  Args:\n",
        "    directory_path (str): Path to documents directory\n",
        "    tokenizer (str): Tokenizer model\n",
        "    chunk_size (int): Maximum tokens per chunk\n",
        "    para_separator (str): Paragraph separator\n",
        "    separator (str): Word separator\n",
        "\n",
        "  Returns:\n",
        "    dict: Document chunks with metadata\n",
        "  \"\"\"\n",
        "\n",
        "  #tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "  documents={}\n",
        "    \n",
        "  for filename in os.listdir(directory_path):\n",
        "    file_path=os.path.join(directory_path,filename)\n",
        "    if not os.path.isfile(file_path):\n",
        "      continue\n",
        "    \n",
        "    with open(file_path,'r',encoding='utf-8') as file:\n",
        "        text=file.read()\n",
        "    \n",
        "    base=os.path.basename(file_path)\n",
        "    sku=os.path.splitext(base)[0]\n",
        "    \n",
        "    doc_id=str(uuid.uuid4())\n",
        "    chunk_collection={}\n",
        "    \n",
        "    potential_paragraphs = re.split(r'\\n\\n', text)\n",
        "    \n",
        "    paragraphs = []\n",
        "    for p in potential_paragraphs:\n",
        "      paragraphs.extend(re.split(r'\\n', p))\n",
        "      \n",
        "    paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "      current_chunk=[]\n",
        "      current_chunk_len=0\n",
        "      \n",
        "      for word in paragraph.split(separator):\n",
        "        word_tokens=len(tokenizer.tokenize(word))\n",
        "        if current_chunk_len+word_tokens<=chunk_size:\n",
        "          current_chunk.append(word)\n",
        "          current_chunk_len+=word_tokens\n",
        "        else:\n",
        "          if current_chunk:\n",
        "            chunk_text=separator.join(current_chunk)\n",
        "            chunk_id=str(uuid.uuid4())\n",
        "            chunk_collection[chunk_id]={\"TEXT\":chunk_text,\"metadata\": {\"filename\": sku}}\n",
        "          \n",
        "          current_chunk=[word]\n",
        "          current_chunk_len=word_tokens\n",
        "          \n",
        "      # Adding the remaining chunk\n",
        "      if current_chunk:\n",
        "        chunk_text=separator.join(current_chunk)\n",
        "        chunk_id=str(uuid.uuid4())\n",
        "        chunk_collection[chunk_id]={\"TEXT\":chunk_text,\"metadata\": {\"filename\": sku}}\n",
        "      \n",
        "    documents[doc_id]=chunk_collection\n",
        "    \n",
        "  return documents\n",
        "          \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model_name= \"BAAI/llm-embedder\"\n",
        "\n",
        "# tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
        "# model=AutoModel.from_pretrained(model_name)\n",
        "\n",
        "def map_document_embeddings(documents,tokenizer,model_name):\n",
        "    mapped_document_db={}\n",
        "    model=AutoModel.from_pretrained(model_name)\n",
        "    model.to(\"cuda\")\n",
        "    \n",
        "    for id, dict_content in documents.items():\n",
        "        mapped_document_db[id]={}\n",
        "        for chunk_id, chunk_content in dict_content.items():\n",
        "            mapped_embeddings={}\n",
        "            text=chunk_content.get(\"TEXT\")\n",
        "            inputs=tokenizer(text,return_tensors=\"pt\",padding=True,truncation=True)\n",
        "            with torch.no_grad():\n",
        "                inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n",
        "                embeddings=model(**inputs).last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "            mapped_embeddings[chunk_id]=embeddings\n",
        "            mapped_document_db[id]=mapped_embeddings\n",
        "            \n",
        "    return mapped_document_db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrival"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_embeddings(query,tokenizer,model_name):\n",
        "    model=AutoModel.from_pretrained(model_name)\n",
        "    model.to(\"cuda\")\n",
        "    query_inputs=tokenizer(query,return_tensors=\"pt\",padding=True,truncation=True)\n",
        "    with torch.no_grad():\n",
        "        query_inputs = {k: v.to(\"cuda\") for k, v in query_inputs.items()}\n",
        "        query_embedding=model(**query_inputs).last_hidden_state.mean(dim=1).squeeze().cpu().tolist()\n",
        "    \n",
        "    return query_embedding\n",
        "\n",
        "def cosine_sim_score(query_embeddings,chunk_embeddings):\n",
        "    normalized_chunk=np.linalg.norm(chunk_embeddings)\n",
        "    normalized_query=np.linalg.norm(query_embeddings)\n",
        "            \n",
        "    if normalized_chunk==0 or normalized_query==0:\n",
        "        score=0\n",
        "    else:\n",
        "        score=np.dot(query_embeddings,chunk_embeddings)/(normalized_query*normalized_chunk)\n",
        "                \n",
        "    return score\n",
        "\n",
        "def get_top_k_scores(query_embeddings,mapped_document_db,top_k):\n",
        "    scores={} \n",
        "    \n",
        "    for doc_id, chunk_dict in mapped_document_db.items():\n",
        "        for chunk_id, chunk_embeddings in chunk_dict.items():\n",
        "            chunk_embedding=np.array(chunk_embeddings)\n",
        "            score=cosine_sim_score(query_embeddings,chunk_embeddings)\n",
        "            scores[(doc_id,chunk_id)]=score\n",
        "    \n",
        "    sorted_scores=sorted(scores.items(),key=lambda item: item[1],reverse=True)[:top_k]\n",
        "    \n",
        "    return sorted_scores\n",
        "\n",
        "def retrieve_top_results(sorted_scores):\n",
        "    top_results=[]\n",
        "    for((doc_id,chunk_id),score) in sorted_scores:\n",
        "        results=(doc_id, chunk_id,score)\n",
        "        top_results.append(results)\n",
        "    \n",
        "    return top_results \n",
        "\n",
        "def save_json(path,data):\n",
        "    with open(path,'w') as f:\n",
        "        json.dump(data,f,indent=4)\n",
        "        \n",
        "def read_json(path):\n",
        "    with open(path,'r') as f:\n",
        "        data=json.load(f)\n",
        "    \n",
        "    return data\n",
        "\n",
        "def retrieve_text(top_results, document):\n",
        "    first_match=top_results[0]\n",
        "    doc_id=first_match[0]\n",
        "    chunk_id=first_match[1]\n",
        "    related_text=document[doc_id][chunk_id]\n",
        "    \n",
        "    return related_text "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LLM response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_llm_response(gemini_model,query,relevant_text):\n",
        "    prompt = f\"\"\"\n",
        "    You are an intelligent search engine. You will be provided with some retrieved context, as well as the users query.\n",
        "\n",
        "    Your job is to understand the request, and answer based on the retrieved context.\n",
        "    Here is context:\n",
        "\n",
        "    <context>\n",
        "    {relevant_text[\"TEXT\"]}\n",
        "    </context>\n",
        "\n",
        "    Question: {query}\n",
        "    \"\"\"\n",
        "    response = gemini_model.generate_content(prompt)\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relevant Text:\n",
            " {'TEXT': \"Make sure your child gets enough sleep. With too little sleep, kids can become hyper, disagreeable, and have extremes in behavior. Getting enough sleep can greatly reduce tantrums. Find out how much sleep is needed at your childâ€™s age. Most kids'\\xa0sleep needs\\xa0fall within a set range of hours based on their age, but each child is unique.\", 'metadata': {'filename': 'behaviuor1'}}\n",
            "\n",
            "LLM Response:\n",
            " Getting enough sleep can greatly reduce tantrums. Kids can become hyper, disagreeable, and have extremes in behavior with too little sleep.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "directory_path=\"./documents\"\n",
        "\n",
        "embedding_model_name=\"BAAI/bge-small-en-v1.5\"\n",
        "tokenizer= AutoTokenizer.from_pretrained(embedding_model_name)\n",
        "\n",
        "chunk_size=200\n",
        "para_separator=\"\\\\n\\\\n\"\n",
        "separator=\" \"\n",
        "top_k=3\n",
        "\n",
        "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
        "model_name=\"gemini-2.0-flash-lite\"\n",
        "gemini_model = genai.GenerativeModel(model_name)\n",
        "\n",
        "documents = chunking(directory_path,tokenizer,chunk_size,para_separator,separator)\n",
        "\n",
        "mapped_document_db=map_document_embeddings(documents,tokenizer,embedding_model_name)\n",
        "\n",
        "save_json('database/doc_store_2.json',documents)\n",
        "save_json('database/vector_store_2.json',mapped_document_db)\n",
        "\n",
        "query = \"why toddlers throw tantrums?\"\n",
        "query_embeddings = compute_embeddings(query, tokenizer, embedding_model_name)\n",
        "sorted_scores = get_top_k_scores(query_embeddings, mapped_document_db, top_k)\n",
        "top_results = retrieve_top_results(sorted_scores)\n",
        "\n",
        "document_data = read_json(\"database/doc_store_2.json\")\n",
        "\n",
        "    # 6. Retrieve Text\n",
        "relavent_text = retrieve_text(top_results, document_data)\n",
        "\n",
        "print(\"Relevant Text:\\n\", relavent_text)\n",
        "\n",
        "    # 7. Generate LLM Response (Uncomment if you have API key)\n",
        "response = generate_llm_response(gemini_model, query, relavent_text)\n",
        "print(\"\\nLLM Response:\\n\", response)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMBjrn9PXxr95iPU4xpI6x8",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "project_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
